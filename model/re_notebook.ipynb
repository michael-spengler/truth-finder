{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c75cbf-6250-4248-a36a-8968ad64bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +\n",
    "# #!pip install transformers\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba56e5-af2e-4e1c-bf33-16d231113caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -\n",
    "\n",
    "def relabel_df(path):\n",
    "    \"\"\"\n",
    "    reads in dataframe from given path,\n",
    "    relabels it for Regression purpose and drops\n",
    "    filtered columns for simplicity\n",
    "    \n",
    "    parameters:\n",
    "        path: preferribly a pathlib object,\n",
    "              string also acceptable (for using colab)\n",
    "              \n",
    "    return:\n",
    "        processed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # scale from 0 (flat out scam)\n",
    "    # to 1 objectivly true and well written\n",
    "    # originally label are provided by pilitico and similar websites\n",
    "    # translation to scale is tuneable and selfmade\n",
    "    dic = {'Mostly True'    : 0.85,\n",
    "           'False'          : 0.10,\n",
    "           'True'           : 1.0,\n",
    "           'Mostly False'   : 0.30, \n",
    "           'Legend'         : 0.0,\n",
    "           'Mixture'        : 0.5,\n",
    "           'Out'            : None,\n",
    "           'Scam'           : 0.0,\n",
    "           }\n",
    "    \n",
    "    # replace original labels and drop uniformative ones\n",
    "    df.replace({\"label\": dic}, inplace=True)\n",
    "    df.dropna(subset=['label'],inplace=True)\n",
    "    df.drop(columns=['author', 'claim', 'title'],\n",
    "             inplace=True)\n",
    "    \n",
    "    # reorders the columns for the bert model\n",
    "    cols = ['text', 'label']\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94ab966-9021-424e-8110-d27a9aa457be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_label_distribution(df):\n",
    "    \n",
    "    names = sorted([str(x) for x in df.label.unique()])\n",
    "    values = df.label.value_counts()\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.bar(names, values, align='center', width=0.5)\n",
    "    plt.suptitle('Class Distribution')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path = Path.cwd().joinpath('data','processed_data.csv')\n",
    "df = relabel_df(path)\n",
    "\n",
    "visulize_label_distribution(df)\n",
    "# dataset overfits towards false news because articles checked on\n",
    "# fact checking sites tend to be false. \"True\" news makes people\n",
    "# less suspicious so it does not get checked\n",
    "\n",
    "def bert_preprocessing(df):\n",
    "    \"\"\"\n",
    "    tokenizes articles and creates the corresponding attention mask for it\n",
    "    \n",
    "    return:\n",
    "        input_ids: tokenized articles\n",
    "        attention_mask: filles the attention mask used on articles shorter then specified\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
    "                                              do_lower_case=True)\n",
    "    sentences = df.text.values\n",
    "    \n",
    "    \n",
    "    # for this version we only use the first 512 characters of a article\n",
    "    # this is to lower computing performance needs\n",
    "    # a why to tokenize hole articles needs to be used for better performance\n",
    "    input_ids = [tokenizer.encode(sent[:512],\n",
    "                                  add_special_tokens=True,\n",
    "                                  max_length=512,\n",
    "                                  pad_to_max_length=True)\n",
    "                 if len(sent)>512\n",
    "                 else tokenizer.encode(sent,\n",
    "                                       add_special_tokens=True,\n",
    "                                       max_length=512,\n",
    "                                       pad_to_max_length=True) \n",
    "                 for sent in sentences]\n",
    "    \n",
    "    attention_masks = []\n",
    "    ## Create a mask of 1 for all input tokens and 0 for all padding tokens\n",
    "    attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720dc212-cb8e-4b42-9f30-29ce9a35eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_ids, masks,labels, batch_size=5):\n",
    "    \"\"\"\n",
    "    splits dataset into train, validation and test set\n",
    "    return:\n",
    "     data loader pipelines for all three subsets\n",
    "    \"\"\"\n",
    "\n",
    "    train_inputs, val_inputs_help, train_labels, val_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.4)\n",
    "    train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.4)\n",
    "\n",
    "    val_inputs, test_inputs, val_labels, test_labels = train_test_split(val_inputs_help, val_labels, random_state=42, test_size=0.5)\n",
    "    val_masks, test_masks, _, _ = train_test_split(val_masks, val_inputs_help, random_state=42, test_size=0.5)\n",
    "\n",
    "    # convert all our data into torch tensors, required data type for our model\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    val_inputs = torch.tensor(val_inputs)\n",
    "    test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    val_labels = torch.tensor(val_labels)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    val_masks = torch.tensor(val_masks)\n",
    "    test_masks = torch.tensor(test_masks)\n",
    "\n",
    "    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "    # with an iterator the entire dataset does not need to be loaded into memory\n",
    "    train_data = TensorDataset(train_inputs,train_masks,train_labels.float())\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "    val_data = TensorDataset(val_inputs,val_masks,val_labels.float())\n",
    "    val_sampler = RandomSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels.float())\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ff386-1874-44f3-86f7-4e80f4890007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(device, pretrained_model_name_or_path=\"bert-base-uncased\", num_labels=1):\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path = pretrained_model_name_or_path, num_labels = num_labels)\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825b2c8-190c-4923-b592-73debfb8eb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(dataloader):\n",
    "    \n",
    "    # Parameters:\n",
    "    lr = 2e-3\n",
    "    adam_epsilon = 1e-8\n",
    "    \n",
    "    epochs = 3\n",
    "    \n",
    "    num_warmup_steps = 0\n",
    "    num_training_steps = len(dataloader)*epochs\n",
    "    \n",
    "    # In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "    optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler\n",
    "    \n",
    "    parameters = {\n",
    "        \"lr\" : lr,\n",
    "        \"adam_epsilon\" : adam_epsilon,\n",
    "        \"epochs\" : epochs,\n",
    "        \"num_warmup_steps\" : num_warmup_steps,\n",
    "        \"num_training_steps\" : num_training_steps,\n",
    "        \"optimizer\" : optimizer,\n",
    "        \"scheduler\" : scheduler\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb5009-ae87-4d82-83c1-9201736f5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, val, model, parameters):\n",
    "    \"\"\"\n",
    "    train: train_dataloader\n",
    "    val: val_dataloader\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = parameters[\"optimizer\"]\n",
    "    scheduler = parameters[\"scheduler\"]\n",
    "    \n",
    "    ## Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "    learning_rate = []\n",
    "\n",
    "    # Gradients gets accumulated by default\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # tnrange is a tqdm wrapper around the normal python range\n",
    "    for _ in trange(1, parameters[\"epochs\"]+1, desc='Epoch'):\n",
    "        print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "        # Calculate total loss for this epoch\n",
    "        batch_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train):\n",
    "            # Set our model to training mode (as opposed to evaluation mode)\n",
    "            model.train()\n",
    "\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # print(b_input_ids[0].dtype ,\"\\n\", b_input_mask[0].dtype, \"\\n\", b_labels[0].dtype)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs[0]\n",
    "            loss = loss.float()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0\n",
    "            # Gradient clipping is not in AdamW anymore\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update learning rate schedule\n",
    "            scheduler.step()\n",
    "\n",
    "            # Clear the previous accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update tracking variables\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            # Calculate the average loss over the training data.\n",
    "            avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "            #store the current learning rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "                learning_rate.append(param_group['lr'])\n",
    "\n",
    "            train_loss_set.append(avg_train_loss)\n",
    "            print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "            \n",
    "            validate(model, val, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503c21d-ca4e-4fae-8665-0cfe3ecd4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \"\"\"\n",
    "    gets metrics for the current model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "     # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits[0].to('cpu').numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        pred_flat = logits.flatten()\n",
    "        labels_flat = label_ids.flatten()\n",
    "        \n",
    "        # placeholder for other metrics. standard deviation would be cool I think\n",
    "        tmp_eval_mse, tmp_eval_variance, tmp_eval_r2_score  = model_score(labels_flat,pred_flat)\n",
    "        \n",
    "        eval_mse += tmp_eval_accuracy\n",
    "        eval_variance += tmp_eval_variance\n",
    "        eval_r2_score += tmp_eval_r2_score\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        \n",
    "    print(F'\\n\\tValidation MSE: {eval_mse/nb_eval_steps}')\n",
    "    print(F'\\n\\tValidation Var: {eval_variance/nb_eval_steps}')\n",
    "    print(F'\\n\\tValidation R2: {eval_r2_score/nb_eval_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78465d04-be9d-4cc8-ac36-ddcde9afc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(labels, prediction):\n",
    "    \n",
    "    mse      = metrics.mean_squared_error(labels, prediction)\n",
    "    variance = metrics.explained_variance_score(labels, prediction)\n",
    "    r2_score = metrics.r2_score(labels, prediction)\n",
    "    \n",
    "    return mse, variance, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51631cf-74fd-449c-bc1c-a58b639c0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "input_ids, attention_masks = bert_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dcf54-4e7f-43f7-b106-07e136cec252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipelines\n",
    "train_dataloader, val_dataloader, test_dataloader = split_dataset(input_ids, attention_masks, df.label.values, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55a974-2541-4162-80fb-75b6d3fb9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e9066c-b8cb-427f-8921-f205d5abbe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = get_model(device, pretrained_model_name_or_path=\"bert-base-uncased\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd6581-38e5-4ad8-8bd0-b51aef835f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = set_parameters(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32645f-e49c-4b8f-9f65-6f4e5125f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_dataloader, val_dataloader, model, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
